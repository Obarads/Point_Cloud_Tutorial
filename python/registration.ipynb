{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registration\n",
    "This section introduces the following registration methods for point clouds, Registration is the task to estimate relative positions and poses between coordinate systems of data (point cloud, depth image, etc.). Examples that registration methods are effective are as follows.\n",
    "\n",
    "- Estimation of the object pose: the registration method estimates a pose of a scanned point cloud with a pose of point cloud sampled from a CAD model.\n",
    "- Integration of point clouds scanned on each location (the following figure): By applying the registration method to estimate a transformation matrix between scenes with overlap parts, we can integrate scene point cloud on a local coordinate system to the global coordinate system.\n",
    "\n",
    "![fig](img/registration.png)\n",
    "\n",
    "This section introduces the following registration methods for point clouds: \n",
    "\n",
    "\n",
    "\n",
    "- SVD (Singular Value Decomposition)\n",
    "- ICP (Iterative Closest Point)\n",
    "- Handcrafted feature and RANSAC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Transformation matrix with SVD (singular value decomposition)\n",
    "This subsection show computing Transformation matrix with SVD. The subsection tutorial use following codes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from tutlibs.sampling import furthest_point_sampling\n",
    "from tutlibs.registration import estimate_transformation\n",
    "\n",
    "import numpy as np\n",
    "import inspect\n",
    "from tutlibs.transformation import Transformation as tr\n",
    "from tutlibs.transformation import TransformationMatrix as tm\n",
    "from tutlibs.utils import single_color\n",
    "from tutlibs.io import Points as io\n",
    "from tutlibs.visualization import JupyterVisualizer as jv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35cad6319534f509bb756364f119cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data acquisition\n",
    "target_xyz, _, _ = io.read('../data/bunny_pc.ply')\n",
    "\n",
    "# source random transformation\n",
    "source_xyz = tr.rotation(\n",
    "    tr.translation(target_xyz, np.array([0.0, 0.1, 0.0])),\n",
    "    'z',\n",
    "    angle=30/180*np.pi\n",
    ")\n",
    "\n",
    "# down sampling\n",
    "ds_indices = furthest_point_sampling(source_xyz, 1000)\n",
    "source_ds_xyz = source_xyz[ds_indices]\n",
    "target_ds_xyz = target_xyz[ds_indices]\n",
    "\n",
    "# create correspondece set between source and target points\n",
    "corr_set = np.tile(np.arange(len(source_ds_xyz))[:, np.newaxis], (1, 2)) # shape: (N, 2)\n",
    "\n",
    "# use svd\n",
    "trans_mat = estimate_transformation(source_ds_xyz, target_ds_xyz, corr_set=corr_set)\n",
    "trans_source_xyz = tm.transformation(source_xyz, trans_mat)\n",
    "\n",
    "# visualize results\n",
    "obj_trans_source_points = jv.point(trans_source_xyz, single_color('#00ff00', len(trans_source_xyz)), [0, 255])\n",
    "obj_target_points = jv.point(target_xyz, single_color('#ffff00', len(target_xyz)), [0, 255])\n",
    "obj_source_points = jv.point(source_xyz, single_color('#ff0000', len(source_xyz)), [0, 255])\n",
    "jv.display([obj_trans_source_points, obj_target_points, obj_source_points])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, red points are source points, yellow points are transformed points and green points are target points. (Yellow points overlap green points)\n",
    "\n",
    "Next, we show `estimate_transformation` function that compute transformation matrix between source and target points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def estimate_transformation(\n",
      "        source:np.ndarray, target:np.ndarray, corr_set:np.ndarray\n",
      "        ) -> np.ndarray:\n",
      "    \"\"\"Estimate transformation with Singular Value Decomposition (SVD).\n",
      "\n",
      "    Args:\n",
      "        source: coordinates of source points, (N, 3)\n",
      "        target: coordinates of target points, (M, 3)\n",
      "        corr_set: correspondence index between source and target (L, 2)\n",
      "\n",
      "    Returns:\n",
      "        rotation_3x3: rotation matrix, (3, 3)\n",
      "        translation_3: translation vector, (3)\n",
      "    \"\"\"\n",
      "\n",
      "    source = source[corr_set[:, 0]]\n",
      "    target = target[corr_set[:, 1]]\n",
      "\n",
      "    centroid_source = np.mean(source, axis=0)\n",
      "    centroid_target = np.mean(target, axis=0)\n",
      "\n",
      "    source = source - centroid_source\n",
      "    target = target - centroid_target\n",
      "\n",
      "    correlation_mat = np.matmul(source.T, target)\n",
      "    u, _, vh = np.linalg.svd(correlation_mat)\n",
      "    rotation_3x3 = np.matmul(vh.T, u.T)\n",
      "    translation_3 = centroid_target - np.matmul(rotation_3x3, centroid_source)\n",
      "\n",
      "    return rotation_3x3, translation_3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(estimate_transformation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICP (Iterative Closest Point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subsection demonstrates object registration with ICP for a object point cloud. The subsection tutorial use following codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutlibs.sampling import furthest_point_sampling\n",
    "from tutlibs.registration import icp\n",
    "\n",
    "import numpy as np\n",
    "import inspect\n",
    "from tutlibs.transformation import Transformation as tr\n",
    "from tutlibs.transformation import TransformationMatrix as tm\n",
    "from tutlibs.utils import single_color\n",
    "from tutlibs.io import Points as io\n",
    "from tutlibs.visualization import JupyterVisualizer as jv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0fcc98d9974711a2dd69a0e94c4e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data acquisition\n",
    "target_xyz, _, _ = io.read('../data/bunny_pc.ply')\n",
    "\n",
    "# source random transformation\n",
    "source_xyz = tr.rotation(\n",
    "    tr.translation(target_xyz, np.array([0.0, 0.1, 0.0])),\n",
    "    'z',\n",
    "    angle=30/180*np.pi\n",
    ")\n",
    "\n",
    "# down sampling\n",
    "ds_indices = furthest_point_sampling(source_xyz, 1000)\n",
    "source_ds_xyz = source_xyz[ds_indices]\n",
    "target_ds_xyz = target_xyz[ds_indices]\n",
    "\n",
    "# use ICP\n",
    "trans_mat = icp(source_ds_xyz, target_ds_xyz, 20)\n",
    "trans_source_xyz = tm.transformation(source_xyz, trans_mat)\n",
    "\n",
    "# visualize results\n",
    "obj_trans_source_points = jv.point(trans_source_xyz, single_color('#00ff00', len(trans_source_xyz)), [0, 255])\n",
    "obj_target_points = jv.point(target_xyz, single_color('#ffff00', len(target_xyz)), [0, 255])\n",
    "obj_source_points = jv.point(source_xyz, single_color('#ff0000', len(source_xyz)), [0, 255])\n",
    "jv.display([obj_trans_source_points, obj_target_points, obj_source_points])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, red points are source points, yellow points are points estimated by ICP and green points are target points. (Yellow points overlap green points)\n",
    "\n",
    "Next, we show `icp` function that estimate transformation matrix between source and target points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def icp(source:np.ndarray, target:np.ndarray, iteration:int,\n",
      "        threshold:float=0.0000001, init_trans_mat:np.ndarray=None) -> np.ndarray:\n",
      "    \"\"\"Iterative Closest Point\n",
      "\n",
      "    Args:\n",
      "        source: coordinates of source points, (N, 3)\n",
      "        target: coordinates of target points, (M, 3)\n",
      "        iteration: icp iteration\n",
      "        threshold: convergence threshold\n",
      "        init_trans_mat: initialinitial transformation matrix, (4, 4)\n",
      "\n",
      "    Return:\n",
      "        trans_mat: (4, 4)\n",
      "    \"\"\"\n",
      "\n",
      "    if init_trans_mat is None:\n",
      "        init_trans_mat = np.identity(4)\n",
      "    trans_mat = init_trans_mat.copy() # transformation matrix\n",
      "\n",
      "    target = target.copy()\n",
      "    source = source.copy()\n",
      "\n",
      "    for _ in range(iteration):\n",
      "        # get previus transformed source xyz\n",
      "        trans_source = tm.transformation_Nx3_with_4x4(source, trans_mat)\n",
      "\n",
      "        # get correspondece set between previus target and transformed source xyz\n",
      "        corr_set = brute_force_matching(trans_source, target)\n",
      "\n",
      "        # get transformation estimation with corr_set\n",
      "        rotation_3x3, translation_3 = estimate_transformation(trans_source,\n",
      "                                                              target,\n",
      "                                                              corr_set)\n",
      "        \n",
      "        # get current transformed source with \n",
      "        new_trans_source = tm.transformation_Nx3_with_3x3_3(trans_source,\n",
      "                                                            rotation_3x3,\n",
      "                                                            translation_3)\n",
      "\n",
      "        r_mat = tm.rotation_3x3_to_4x4(rotation_3x3)\n",
      "        t_mat = tm.translation_3_to_4x4(translation_3)\n",
      "        trans_mat = tm.composite_4x4([t_mat, r_mat, trans_mat])\n",
      "\n",
      "        if np.sum(np.abs(new_trans_source - trans_source)) < threshold:\n",
      "            break\n",
      "\n",
      "    return trans_mat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(icp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, red points are source points, yellow points are points estimated by ICP and green points are target points. (Yellow points overlap green points.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handcrafted feature and RANSAC (RANdom SAmple Consensus)\n",
    "This subsection demonstrates scene registration with handcrafted feature and RANSAC. The subsection tutorial use following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from tutlibs.sampling import furthest_point_sampling, voxel_grid_sampling\n",
    "from tutlibs.registration import feature_ransac\n",
    "from tutlibs.feature import PointFeatureHistograms as pfh\n",
    "from tutlibs.normal_estimation import normal_estimation, normal_orientation\n",
    "\n",
    "import numpy as np\n",
    "from tutlibs.transformation import Transformation as tr\n",
    "from tutlibs.transformation import TransformationMatrix as tm\n",
    "from tutlibs.io import Points as io\n",
    "from tutlibs.visualization import JupyterVisualizer as jv\n",
    "from tutlibs.utils import single_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7025fd27564f0d968e048c9bb4a317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data acquisition\n",
    "target_xyz, _, data = io.read('../data/bunny_pc.ply')\n",
    "target_normal = np.stack([data['nx'], data['ny'], data['nz']], axis=-1)\n",
    "# target_normal = normal_estimation(target_xyz, k=15)\n",
    "\n",
    "# source random transformation\n",
    "rot_angle = 30/180*np.pi\n",
    "source_xyz = tr.rotation(\n",
    "    tr.translation(target_xyz, np.array([0.0, 0.1, 0.0])),\n",
    "    'z',\n",
    "    rot_angle\n",
    ")\n",
    "\n",
    "# down sampling (keypoint)\n",
    "ds_indices = furthest_point_sampling(target_xyz, 1000)\n",
    "source_ds_xyz = source_xyz[ds_indices]\n",
    "target_ds_xyz = target_xyz[ds_indices]\n",
    "\n",
    "# normals\n",
    "target_ds_normal = target_normal[ds_indices]\n",
    "source_ds_normal = tr.rotation(target_ds_normal, 'z', angle=rot_angle)\n",
    "\n",
    "# extract Handcrafted feature\n",
    "source_ds_pfh = pfh.compute(source_ds_xyz, source_ds_normal, 0.05)\n",
    "target_ds_pfh = pfh.compute(target_ds_xyz, target_ds_normal, 0.05)\n",
    "\n",
    "# feature matching\n",
    "trans_mat = feature_ransac(source_ds_xyz, target_ds_xyz, source_ds_pfh,\n",
    "                           target_ds_pfh, 3, 1000, 0.075)\n",
    "trans_source_xyz = tm.transformation(source_xyz, trans_mat)\n",
    "\n",
    "# visualize results\n",
    "obj_trans_source_points = jv.point(trans_source_xyz, single_color('#00ff00', len(trans_source_xyz)), [0, 255])\n",
    "obj_target_points = jv.point(target_xyz, single_color('#ffff00', len(target_xyz)), [0, 255])\n",
    "obj_source_points = jv.point(source_xyz, single_color('#ff0000', len(source_xyz)), [0, 255])\n",
    "jv.display([obj_trans_source_points, obj_target_points, obj_source_points])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3634c03ba3d4fe59cd5bad0d7c3b142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data acquisition\n",
    "target_xyz, _, _ = io.read('../data/redwood_3dscan_1.ply')\n",
    "target_ds_xyz = voxel_grid_sampling(target_xyz, 0.05)\n",
    "target_ds_normal = normal_estimation(target_ds_xyz, k=15)\n",
    "target_ds_normal = normal_orientation(target_ds_xyz, target_ds_normal)\n",
    "\n",
    "# source random transformation\n",
    "source_xyz, _, _ = io.read('../data/redwood_3dscan_2.ply')\n",
    "source_ds_xyz = voxel_grid_sampling(source_xyz, 0.05)\n",
    "source_ds_normal = normal_estimation(source_ds_xyz, k=15)\n",
    "source_ds_normal = normal_orientation(source_ds_xyz, source_ds_normal)\n",
    "\n",
    "# extract Handcrafted feature\n",
    "source_ds_pfh = pfh.compute(source_ds_xyz, source_ds_normal, 0.25)\n",
    "target_ds_pfh = pfh.compute(target_ds_xyz, target_ds_normal, 0.25)\n",
    "\n",
    "# feature matching\n",
    "trans_mat = feature_ransac(source_ds_xyz, target_ds_xyz, source_ds_pfh,\n",
    "                           target_ds_pfh, 3, 100, 0.075)\n",
    "trans_source_xyz = tm.transformation(source_ds_xyz, trans_mat)\n",
    "\n",
    "# visualize results\n",
    "obj_target_points = jv.point(target_ds_xyz, single_color('#00ff00', len(target_ds_xyz)))\n",
    "obj_trans_source_points = jv.point(trans_source_xyz, single_color('#ffff00', len(trans_source_xyz)))\n",
    "jv.display([obj_trans_source_points, obj_target_points])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
