{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "本章では、点群に対する深層学習手法について紹介します。Deep learning is a method of learning optimal features from given data. This method has some limitations, such as the need to prepare data that matches the target task, but if the limitations can be met, it has the potential to achieve better results than other methods. For this reason, methods using this deep learning approach have been actively researched in recent years. \n",
    "点群に対する深層学習手法はクラス分類やセグメンテーション等様々なタスクに対して提案されています。以下の様に、これらの点群の深層学習手法の根幹は、画像に対する深層学習手法等と似ていることが多いです。ただし、画像等と比べて点群が決まった構造を持たない表現であるため、前処理や深層学習のモデルが点群特有である場合があります。本章では、点群深層学習で一般的なタスクであるクラス分類タスクに基づいた深層学習モデルの紹介を行います。\n",
    "\n",
    "本章では、代表的で実装が比較的簡単な手法を紹介します。紹介する手法は以下の通りです。\n",
    "\n",
    "- VoxNet\n",
    "- PointNet\n",
    "- PointNet++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VoxNet\n",
    "VoxNetは占有モデルを入力とするネットワークです。VoxNetでは点群を占有モデルへ変換し、その占有モデルに基づいてクラス分類を行うことができます。この手法の利点は以下の通りです。\n",
    "\n",
    "- 3DCNNsの利用: グリッドに沿った表現である占有モデルを利用するため、2DCNNsを拡張した3DCNNsを点群等の立体的な表現へ適用することが可能となります。これにより、2DCNNsの手法の応用が用意となります。\n",
    "- 効率的な計算: グリッドに沿って近傍情報の畳み込みを行うことが可能となる。グリッドに沿った畳み込みを行わない場合、kNN等を利用して近い点を探す必要があり、処理時間が増える可能性が高い。ダウンサンプリング処理もグリッドに沿って効率的に可能である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "\n",
    "from tutlibs.dl.PointNet import PointNetClassification\n",
    "from tutlibs.dl.dataset import ModelNet40Dataset\n",
    "from tutlibs.dl.loss import feature_transform_regularizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    epochs = 250\n",
    "    device = 0\n",
    "\n",
    "    dataset = ModelNet40Dataset(\"../data/modelnet40_ply_hdf5_2048/\")\n",
    "\n",
    "    model = PointNetClassification(40)\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=20,\n",
    "        gamma=0.7,\n",
    "    )\n",
    "\n",
    "    loss_ce = nn.CrossEntropyLoss()\n",
    "    loss_ftr = feature_transform_regularizer\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch {}/{}:\".format(epoch, epochs))\n",
    "        loader = DataLoader(dataset, 32, shuffle=True)\n",
    "        model.train()\n",
    "        for data in tqdm(loader, desc=\"batch\", ncols=60):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            point_clouds, gt_labels = data\n",
    "            point_clouds = torch.transpose(point_clouds, 1, 2).to(\n",
    "                device, dtype=torch.float32\n",
    "            )\n",
    "            gt_labels = gt_labels.to(device, dtype=torch.long)\n",
    "\n",
    "            pred_labels, _, feature_transformation_matrix = model(point_clouds)\n",
    "\n",
    "            loss = loss_ce(pred_labels, gt_labels)\n",
    "            if feature_transformation_matrix is not None:\n",
    "                loss += loss_ftr(feature_transformation_matrix) * 0.001\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "        },\n",
    "        \"model_path.pth\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PointNet\n",
    "PointNet[Qi et al. 2017a]は点群から点ごとの特徴(local (pointwise?) feature)と点群全体の特徴(global feature)を抽出することが可能なネットワークです。点群を適切に処理可能にするため、以下の問題に対して以下の構造を提案した(点群の問題については[characteristic.ipynb](characteristic.ipynb))。\n",
    "\n",
    "- 点の順不同性: Point-wise convolution layerを採用することで個々の点が持つ点のみを畳み込む。Poolingは点方向に行う((N, 1024) -> (1024))。\n",
    "- オブジェクトのランダム向き: TransformationNetworkを入力点群と特徴量に対して設けた。\n",
    "\n",
    "PointNetアーキテクチャは以下の通り。\n",
    "\n",
    "![pointnet](img/pointnet.png)\n",
    "\n",
    "PointNetを用いた推論は以下の通り。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "\n",
    "from tutlibs.dl.PointNet import PointNetClassification\n",
    "from tutlibs.dl.dataset import (\n",
    "    ModelNet40Dataset,\n",
    "    rotate_point_cloud,\n",
    "    jitter_point_cloud,\n",
    ")\n",
    "from tutlibs.dl.loss import feature_transform_regularizer\n",
    "from tutlibs.dl.utils import t2n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|████████████████| 78/78 [00:00<00:00, 298.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 25.85089111328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    device = 0\n",
    "    output_dir_path = \"outputs/PointNet/\"\n",
    "    dataset_dir_path = \"../data/modelnet40_ply_hdf5_2048/\"\n",
    "    num_points = 1024\n",
    "    num_classes = 40\n",
    "\n",
    "    os.makedirs(output_dir_path, exist_ok=True)\n",
    "\n",
    "    dataset = ModelNet40Dataset(dataset_dir_path, mode=\"test\")\n",
    "\n",
    "    model = PointNetClassification(40)\n",
    "    model = model.to(device=device)\n",
    "    checkpoint = torch.load(os.path.join(output_dir_path, \"model_path.pth\"))\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "    loader = DataLoader(dataset, 32, shuffle=False)\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader, desc=\"test\", ncols=60):\n",
    "            point_clouds, gt_labels = data\n",
    "\n",
    "            point_clouds = point_clouds[:, 0:num_classes]\n",
    "            point_clouds = torch.transpose(point_clouds, 1, 2).to(\n",
    "                device, dtype=torch.float32\n",
    "            )\n",
    "            gt_labels = gt_labels.to(device, dtype=torch.long)\n",
    "\n",
    "            net_output, _, _ = model(point_clouds)\n",
    "            pred_labels = torch.argmax(net_output, dim=1)\n",
    "            results.append(pred_labels == gt_labels)\n",
    "\n",
    "    results = torch.cat(results, dim=0)\n",
    "    acc = torch.sum(results) / len(results) * 100\n",
    "    print(f\"accuracy: {acc}\")\n",
    "\n",
    "\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練は以下の通り。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training epoch: 100%|█████| 250/250 [47:25<00:00, 11.38s/it]\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    epochs = 250\n",
    "    device = 0\n",
    "    output_dir_path = \"outputs/PointNet\"\n",
    "    dataset_dir_path = \"../data/modelnet40_ply_hdf5_2048/\"\n",
    "    num_points = 1024\n",
    "    num_classes = 40\n",
    "\n",
    "    os.makedirs(output_dir_path, exist_ok=True)\n",
    "\n",
    "    dataset = ModelNet40Dataset(dataset_dir_path, mode=\"train\")\n",
    "\n",
    "    model = PointNetClassification(num_classes)\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=20,\n",
    "        gamma=0.7,\n",
    "    )\n",
    "\n",
    "    loss_ce = nn.CrossEntropyLoss()\n",
    "    loss_ftr = feature_transform_regularizer\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"training epoch\", ncols=60):\n",
    "        loader = DataLoader(dataset, 32, shuffle=True)\n",
    "        model.train()\n",
    "        for data in loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            point_clouds, gt_labels = data\n",
    "\n",
    "            point_clouds = point_clouds[:, 0:num_points]\n",
    "            point_clouds = rotate_point_cloud(t2n(point_clouds))\n",
    "            point_clouds = jitter_point_cloud(point_clouds)\n",
    "\n",
    "            point_clouds = torch.tensor(\n",
    "                point_clouds, dtype=torch.float32, device=device\n",
    "            ).transpose(1, 2)\n",
    "            gt_labels = gt_labels.to(dtype=torch.long, device=device)\n",
    "\n",
    "            net_output, _, feature_transformation_matrix = model(point_clouds)\n",
    "\n",
    "            loss = loss_ce(net_output, gt_labels)\n",
    "            if feature_transformation_matrix is not None:\n",
    "                loss += loss_ftr(feature_transformation_matrix) * 0.001\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "        },\n",
    "        os.path.join(output_dir_path, \"model_path.pth\"),\n",
    "    )\n",
    "\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PointNet++\n",
    "PointNet++[Qi et al. 2017b]は、点の局所領域の特徴を抽出する機構を持つネットワークです。PointNetでは、点ごとの特徴のみを畳みこんでいましたが、本提案ではhybrid searchを利用して点ごとに近傍点を求め、その近傍点間の関係性を畳みこむ機構を持ちます。本提案はPointNetと比較した利点が以下の通りです。\n",
    "\n",
    "- 局所領域の畳み込み: hybrid searchを用いて近傍点とそのクエリの相対位置を用いたプーリングと畳み込みを行います。handcrafted feature等でも利用されていたように、局所領域の関係性は点ごとの識別的な特徴を持つうえで重要とされています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "\n",
    "from tutlibs.dl.PointNet import PointNetClassification\n",
    "from tutlibs.dl.dataset import ModelNet40Dataset\n",
    "from tutlibs.dl.loss import feature_transform_regularizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    epochs = 250\n",
    "    device = 0\n",
    "\n",
    "    dataset = ModelNet40Dataset(\"../data/modelnet40_ply_hdf5_2048/\")\n",
    "\n",
    "    model = PointNetClassification(40)\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=20,\n",
    "        gamma=0.7,\n",
    "    )\n",
    "\n",
    "    loss_ce = nn.CrossEntropyLoss()\n",
    "    loss_ftr = feature_transform_regularizer\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch {}/{}:\".format(epoch, epochs))\n",
    "        loader = DataLoader(dataset, 32, shuffle=True)\n",
    "        model.train()\n",
    "        for data in tqdm(loader, desc=\"batch\", ncols=60):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            point_clouds, gt_labels = data\n",
    "            point_clouds = torch.transpose(point_clouds, 1, 2).to(\n",
    "                device, dtype=torch.float32\n",
    "            )\n",
    "            gt_labels = gt_labels.to(device, dtype=torch.long)\n",
    "\n",
    "            pred_labels, _, feature_transformation_matrix = model(point_clouds)\n",
    "\n",
    "            loss = loss_ce(pred_labels, gt_labels)\n",
    "            if feature_transformation_matrix is not None:\n",
    "                loss += loss_ftr(feature_transformation_matrix) * 0.001\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "        },\n",
    "        \"model_path.pth\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Maturana, Daniel, and Sebastian Scherer. 2015. “VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition.” In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE. https://doi.org/10.1109/iros.2015.7353481.\n",
    "- Qi, Charles R., Hao Su, Kaichun Mo, and Leonidas J. Guibas. 2017. “Pointnet: Deep Learning on Point Sets for 3d Classification and Segmentation.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 652–60.\n",
    "- Qi, Charles R., Li Yi, Hao Su, and Leonidas J. Guibas. 2017. “PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1706.02413."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
