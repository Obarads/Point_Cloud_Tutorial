{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characteristic\n",
    "A point cloud is a set of points with coordinates, and different from an image and meshes. In this section, we introduce the following characteristics of the point cloud. \n",
    "- Properties of points\n",
    "- Point density\n",
    "- Invariance to rigid transformation\n",
    "- Unordered points\n",
    "- Definition of Neighborhood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of points\n",
    "Points have coordinats and the coordinates are XYZ values. Points have coordinates, and the coordinates are XYZ values. The point position depends on the coordinates.\n",
    "\n",
    "Also, points can have colors and normals, etc.\n",
    "- **Color (RGB)**: the color is used when displaying points on viewers, or for point cloud processing.\n",
    "- **Normals**: we can get the surface of a 3D object or detect planes from normals.\n",
    "- **Intensity**:  \"intensity\" is the laser reflection intensity, and depends on the reflectivity of the object surface that the laser hits. Intensity can be used for feature extraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point density\n",
    "There are density differences of points in a point cloud because points are not arranged regularly. An example of a point density difference is a point distribution acquired from a sensor. You can confirm a point cloud acquired from a sensor by following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c660aefae334d1ebe328fe4038d0560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tutlibs.io import Points\n",
    "from tutlibs.visualization import JupyterVisualizer as jv\n",
    "coords, colors, _ = Points.read(\"../data/kitti_sample.ply\")\n",
    "    obj_points = jv.point(coords, colors, point_size=0.07)\n",
    "jv.display([obj_points])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point colors in outputs show the distance from the sensor to the point, and red is the closest. Let's compare the surrounding of the orange and blue points. You can see that the orange point has a dense point around it, while the blue point is sparse. The density differences of points affect point cloud processing such as nearest neighbor search, downsampling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invariance to rigid transformations\n",
    "Point cloud shape is invariance to rotation and translation (rigid transformation). In some tasks, we must consider rotation and translation of a point cloud. For example, in the classification task, we must prepare a method that is invariant to an object's direction because input object data do not have a consistent direction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unordered points\n",
    "Points on a point cloud do not depend on order. Therefore, a point cloud shape is immutable even if points are reordered. We can confirm this characteristic by the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tutlibs.visualization import JupyterVisualizer as jv\n",
    "from tutlibs.utils import single_color\n",
    "from tutlibs.transformation import Transformation as tr\n",
    "from tutlibs.io import Points as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24706d70daa647b486ee97ee18e66dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coords, _, _ = io.read('../data/bunny_pc.ply')\n",
    "num_points = len(coords)\n",
    "\n",
    "# reorder\n",
    "idxs = np.random.choice(np.arange(num_points), num_points, replace=False)\n",
    "reordered_coords = coords[idxs]\n",
    "\n",
    "# translation to compare an origin\n",
    "reordered_coords = tr.translation(reordered_coords, np.array([1, 0, 0]))\n",
    "\n",
    "obj_points = jv.point(coords, single_color(\"#ff0000\", num_points))\n",
    "obj_random_points = jv.point(reordered_coords, single_color(\"#00ff00\", num_points))\n",
    "jv.display([obj_points, obj_random_points])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output shows the original point cloud (green) and the reordered point cloud (red). As you can see, the point shape have no change. If we do not use point indices obtained from a depth camera image, LiDAR, etc., it is necessary to consider processing that does not depend on point order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Neighborhood\n",
    "A point cloud does not have adjacencies between points. In the point cloud processing, we might need to define adjacency to use convolutions and handcrafted features, etc. We can access adjacencies of pixels by a grid index, on the other hand, we need to define adjacencies of points from point coordinates and distances. This adjacency depends on the nearest neighbor search. For example, kNN (k Nearest Neighbors) and radius nearest neighbor define neighborhoods by the number of points (k) and range (r), respectively, so adjacencies are different. We show kNN and radius nearest neighbors (red is neighbors, blue is other):\n",
    "\n",
    "![neighbors](img/neighbors.png)\n",
    "\n",
    "The definition make the following difference.\n",
    "- If we get k neighbors from N query points with kNN, kNN output an array with ($N \\times k$). However, we get far points if there are no points close to queries.\n",
    "- We can get neighbors within r (radius) by radius nearest neighbor. However, the number of neighbors may be different by queries.\n",
    "\n",
    "As shown in the above example, the definition of neighbors has a significant impact on the subsequent processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "4b28c7e065dc872b238847722c3716b3aa1fa0ead1eaaaca0154caa64a92cae9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('pct_cpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
