{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 点群の特性"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Abstract\n",
    "- 点群の特性について説明します。\n",
    "- コードも含まれますが、点群を視覚化するためであるため、コードの内容に注目する必要はありません。\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "点群は座標値を持つ点の集合であり、点群の点の一つ一つには座標値があります。この章では、この点群という表現の特性について、他の表現(主に画像)との比較も行いながら紹介、説明します。この章で説明する特徴は節ごとに分けてあります:\n",
    "\n",
    "- 点群の点の情報\n",
    "- 不規則な点の並び\n",
    "  - 疎もしくは密な表現\n",
    "  - 点間の隣接関係の定義\n",
    "- 剛体変換に対する不変性 (タスクによる)\n",
    "- 点の順不同に対する形状不変性\n",
    "\n",
    "ここでは、これらを一つづつ紹介、説明していきます。\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 点群の点の情報\n",
    "点群の点は座標値以外に、色情報や法線などの情報を持ち合わせます。持ち合わせる情報としては以下の情報が挙げられます。\n",
    "\n",
    "- **色 (RGB)**: 色情報はビュワーなどで点を表示する際に点に着色するために使用されたり、点群処理に利用されたりします。ただし、色情報の取得と点群情報(3次元座標情報)の取得はそれぞれ別のセンサーで行われるため、点に割り振られる色がずれることがあります。\n",
    "- **法線**: 法線から立体オブジェクトの表面を推定したり(再構築タスクに最適)、平面検出を行ったりする際に利用されたりします。法線の取得方法の例として、点群から直接取得する、他の3D表現であるメッシュ表現から得る等が挙げられます。\n",
    "- **反射強度**: LiDARなどのレーザーセンサーで得られる点ごとのレーザーの反射強度値。レーザーが当たったオブジェクトの反射率にも左右されるため、特徴抽出に使用されたりします。\n",
    "\n",
    "注意点として、この情報は他の3D表現でも持ち合わせることがあります。\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 不規則な並び\n",
    "点の位置は座標値で決まるため、点は画像のピクセルのように規則的な並びを持ちません。このような特性を持つことにより、いくつか留意することがあります。以下でその説明と関連するコードを示します。\n",
    "\n",
    "本subsectionで使用するパッケージは以下のとおりです。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import k3d\n",
    "import numpy as np\n",
    "from tutlibs.visualization import Points as visualizer\n",
    "from tutlibs.io import Points as io\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 疎もしくは密な表現\n",
    "ここで言う疎と密は、ある範囲内の点の密度のことを指します。この密度についてわかりやすい例として、センサーから取得した点群が当てはまります。実際のセンサーから取得したデータを下のコードの実行で確認できます。\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Read a point cloud\n",
    "xyz, rgb, data = io.read('../data/kitti_sample.ply')\n",
    "\n",
    "# Visualize a point cloud\n",
    "visualizer.k3d(xyz, rgb, color_range=[0, 255], point_size=0.3)\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "265ec938d2ff43fcb1fb9ce8633765c5"
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "上の出力はセンサーから点までの距離を色の変化によって示しています。赤がセンサーから近く、徐々に青になることでセンサーから距離が離れていることを示しています。ここで、センサーからの距離によって点がまばらになっていくことがわかると思います。\n",
    "これは、距離が遠くなるにつれて、オクルージョンやセンサー自身の解像度などの複数の要因が影響してまばらなになっています(この要因についてはここでは詳細に説明しません)。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 要素間の明確なつながりを持たない表現\n",
    "\n",
    "点群は点の集合であり、点と点の明確な隣接関係を持ちません。ここで言う「隣接関係」とは、要素と要素のつながりのことを指します。  \n",
    "例えば、グラフ表現であるなら、グラフの要素である頂点は辺というつながりによって他の頂点とのつながりを持ちます。画像であるなら、画像の要素であるピクセルは必ず隣にピクセルがあり、それとつながりを持ちます。  これらの表現に比べて、点群の点は他の点との明確な隣接関係があらかじめ設けられていません。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "この隣接関係が明確でない特性は、問題を生むことがあります。例えば、隣接する要素を必要とする畳み込みアルゴリズムの適用を阻むという問題があります。これらの問題を解決するには以下のような方法が挙げられます。\n",
    "\n",
    "1. 点群の各点に対してk近傍(kNN)やボールクエリ等を実施し、点の近くにある点との隣接関係を持たせる。\n",
    "2. 点群をボクセル表現に変換する。\n",
    "\n",
    "1番目の方法は、点の周りの情報を収集するためによく使われます。例えば、handcrafted featureを生成する際は、点の周りにある他の点の情報を参照するためにkNNが使用されます。また、深層学習手法などの畳込み機構を設けた処理では、同様に周りの点の特徴を畳み込むために使用されます。\n",
    "2番目の方法は、点群を画像の様なグリッドに沿ったデータフォーマットに変換することができます。これにより、例えば深層学習では画像用の2D CNNを拡張した3D CNNの適用が可能になりました。\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 剛体変換に対する不変性 (タスクによる)\n",
    "点群に対して剛体変換を用いることで点群が持つ形状を崩さずに姿勢の変更や平行移動が可能です。タスクによっては剛体変換された点群に対する処理結果が変化しないようにする必要があります。例えばオブジェクトに対する分類タスクでは、与えられる入力点群の姿勢が一定ではない可能性があり、一定でない場合はどの様な姿勢に対しても不変な予測を行う必要があります。\n",
    "「タスクによる」とあるのは、姿勢や位置そのものを予測するタスク(例:姿勢推定)などが当てはまらないためです。これらのタスクは、与えられたデータの姿勢や位置に合わせて予測を行う必要があります。\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 点の順不同による形状不変性\n",
    "\n",
    "点群は画像や文字列の様に要素に順序があるわけではないため、点の入力順序を変更したとしても点群が示す形状は変わりません。例えば、オブジェクト点群内にある点の順序をランダムに入れ替えたとしても、オブジェクトの形状は変わりません。\n",
    "例として、オリジナルの点群と点の順序を入れ替えた点群をそれぞれ表示します。この表示で使うパッケージは以下のとおりです。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from tutlibs.visualization import Points as visualizer\n",
    "from tutlibs.io import Points as io"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "点の順序入れ替え前と後を表示するコードは以下のとおりです。X軸で0~1の間にあるウサギの点群が入れ替え前、1~2の間にあるウサギの点群が入れ替え後です。  \n",
    "Note: 見比べを行うために入れ替え後の点群の位置を平行移動させています。  \n",
    "問題点: 平行移動による形状が変わらないことへの言及がない"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "xyz, rgb, _ = io.read('../data/bunny_pc.ply')\n",
    "\n",
    "idxs = np.random.choice(np.arange(len(xyz)), len(xyz), replace=False)\n",
    "random_xyz = xyz[idxs]\n",
    "random_rgb = rgb[idxs]\n",
    "\n",
    "random_xyz[:, 0] += 1.0 # slide x\n",
    "visualizer.k3d(np.vstack([xyz, random_xyz]), np.vstack([rgb, random_rgb]), color_range=[0, 255], point_size=0.02)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "207f4a8a45e5433294469452c5314d25"
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "tags": [
     "Basic"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "上の出力から見てわかるように、ウサギの形状も色も変わってないことが示されていると思います。  \n",
    "実際にこれらの点群を処理する場合、その手法の出力は点群の点の順序に依存しない処理方法が必要となります。例えば機械学習の分野の分類タスクを解く場合、ある点群AとAの点の順序を変えた点群Bを機械学習モデルに入力したとしても、AとBの分類予測ラベルは同じでなければいけません。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reference\n",
    "- [Naoya Chiba, 2019. \"点群深層学習 Meta-study.\" URL:https://www.slideshare.net/naoyachiba18/metastudy. (access:2021/08/13)](https://www.slideshare.net/naoyachiba18/metastudy)\n",
    "- [A Geiger, P Lenz, C Stiller, and R Urtasun. 2013. Vision meets robotics: The KITTI dataset. Int. J. Rob. Res. 32, 11 (September 2013), 1231–1237. DOI:https://doi.org/10.1177/0278364913491297](https://dl.acm.org/doi/10.1177/0278364913491297)\n",
    "- [Stanford bunny](http://graphics.stanford.edu/data/3Dscanrep/)\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "4b28c7e065dc872b238847722c3716b3aa1fa0ead1eaaaca0154caa64a92cae9"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('pct_cpu': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}